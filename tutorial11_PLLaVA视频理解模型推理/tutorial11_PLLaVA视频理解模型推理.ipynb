{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial11: PLLaVA视频理解模型推理\n",
    "\n",
    "建议在SCOW AI集群运行本教程。推荐使用[PLLaVA-NPU库](https://github.com/SunnyMass/PLLaVA-NPU)的代码。\n",
    "\n",
    "本节旨在使用 [pllava-7b](https://huggingface.co/ermu2001/pllava-7b) 模型展示多模态视频理解模型的推理。\n",
    "\n",
    "分以下几步来实现：\n",
    "1. 环境安装\n",
    "2. 下载模型\n",
    "3. 3D池化算子适配\n",
    "4. 模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境安装\n",
    "\n",
    "建议使用1张910B NPU运行本教程。\n",
    "\n",
    "按照如下方式创建conda虚拟环境，并安装所需库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conda create -n pllava python=3.10\n",
    "conda activate pllava\n",
    "pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 下载模型\n",
    "\n",
    "按照如下方式下载pllava-7b模型。\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download ermu2001/pllava-7b --local-dir pllava-7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3D池化算子适配\n",
    "\n",
    "原始代码问题分析：\n",
    "\n",
    "RuntimeError: adaptive_avg_pool3d only support D=1 && H=1 && W=1 current!该问题指出，NPU不支持3D池化，即不支持torch._C._nn.adaptive_avg_pool3d自适应尺寸。如果解决这种报错，必须要修改尺寸，这会导致模型推理时，出现错误的映射并无法对齐，从而导致幻觉等问题出现。如果改成2D池化或者用其他方式表征时域，都无法改善这种幻觉问题，而在GPU上推理能得到准确结果。因此需要从本质上解决这个3D池化函数适配问题。\n",
    "AdaptiveAvgPool3d((T_out, H_out, W_out)) 表示：在三维空间（时间 + 空间）上，自动将每段 video（形状 [C, T, H, W]）平均池化到指定形状 [C, T_out, H_out, W_out]。如果输入是 [B, C, T=8, H=14, W=14]，设置 --pooling_shape 4-12-12，就会变成：输出形状 = [B, C, 4, 12, 12]，从而稀疏化token个数。\n",
    "\n",
    "\n",
    "解决方案：\n",
    "\n",
    "由于 NPU 不支持包括 AdaptiveAvgPool3d 在内的多种 3D 池化算子，因此我们手动实现一个兼容 NPU 的 3D 池化函数，用于在不改变数值语义的前提下，完成时间维度和空间维度的降采样，确保推理过程与 GPU 上一致，避免出现映射错乱或语义幻觉等问题。该函数完全复现了 AdaptiveAvgPool3d 的行为，前提是输入的时间、空间尺寸能整除目标尺寸。适用于大多数实际配置，同时在 NPU 上高效可用，确保视觉特征 token 数在时空维度的稀疏化过程保持一致。\n",
    "\n",
    "在**PLLaVA/models/pllava/modeling_pllava.py**文件中修改，把class PllavaMultiModalProjector(nn.Module):中的self.pooling替换为下面的自定义实现。该实现已经验证在GPU和NPU的输出保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def adaptive_avg_pool3d_manual(self, x, output_size):\n",
    "        \"\"\"\n",
    "        x: [B, C, D, H, W]\n",
    "        output_size: (d_out, h_out, w_out)\n",
    "        替代 AdaptiveAvgPool3d，NPU 兼容\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        d_out, h_out, w_out = output_size\n",
    "        print(output_size)\n",
    "        assert D % d_out == 0 and H % h_out == 0 and W % w_out == 0, \"Input size must be divisible by output size\"\n",
    "\n",
    "        kd = D // d_out\n",
    "        kh = H // h_out\n",
    "        kw = W // w_out\n",
    "\n",
    "        # reshape 成 6维：将 D/H/W 分成 avg block 块 + 块内元素\n",
    "        x = x.view(B, C, d_out, kd, h_out, kh, w_out, kw)  # [B, C, d_out, kd, h_out, kh, w_out, kw]\n",
    "        x = x.mean(dim=(3, 5, 7))  # 对 kd, kh, kw 三个维度做均值\n",
    "        return x  # shape [B, C, d_out, h_out, w_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型推理\n",
    "\n",
    "PLLaVA-7B 是一个面向视频理解任务的开源多模态聊天模型。该模型在图像大模型的基础上，进一步引入视频指令数据进行微调，具备视频内容理解与问答能力。其底层语言模型为 llava-hf/llava-v1.6-vicuna-7b-hf，采用 Transformer 架构，具备自回归生成能力。\n",
    "\n",
    "推理过程中使用 npu-smi info 命令可以查看 NPU 运行情况。\n",
    "\n",
    "推荐直接使用[PLLaVA-NPU库](https://github.com/SunnyMass/PLLaVA-NPU)的代码运行。\n",
    "\n",
    "运行命令：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python run_demo.py   --video_path path_to_1-2.mp4   --prompt \"describe this video in detail\"   --pretrained_model_name_or_path path_to_pllava7b   --weight_dir path_to_pllava7b   --use_lora   --num_frames 16   --conv_mode plain   --max_new_tokens 128  --video_caption(如果是做视频caption任务就加上，如果是其他视频理解任务就不加)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tasks.eval.eval_utils import conv_templates, ChatPllava\n",
    "from tasks.eval.model_utils import load_pllava\n",
    "\n",
    "SYSTEM = \"\"\"You are a powerful Video Magic ChatBot, a large vision-language assistant. \n",
    "You are able to understand the video content that the user provides and assist the user in a video-language related task.\n",
    "The user might provide you with the video and maybe some extra noisy information to help you out or ask you a question. Make use of the information in a proper way to be competent for the job.\n",
    "### INSTRUCTIONS:\n",
    "1. Follow the user's instruction.\n",
    "2. Be critical yet believe in yourself.\n",
    "\"\"\"\n",
    "SYSTEM2 = \"\"\"\n",
    "Describe this video. Pay attention to all objects in the video. The description should be useful for AI to re-generate the video. The description should be no more than six sentences. Here are some examples of good descriptions: 1. A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about. 2. Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field. 3. Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliff’s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.\n",
    "\"\"\"\n",
    "\n",
    "def load_video(video_path, num_segments=4, resolution=336):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = [int(i * total_frames / num_segments) for i in range(num_segments)]\n",
    "\n",
    "    frames = []\n",
    "    resize = transforms.Resize((resolution, resolution))\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx in indices:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame)\n",
    "            pil_img = resize(pil_img)\n",
    "            frames.append(pil_img)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--video_path', type=str, required=True)\n",
    "    parser.add_argument('--prompt', type=str, required=True)\n",
    "    parser.add_argument('--num_frames', type=int, default=4)\n",
    "    parser.add_argument('--pretrained_model_name_or_path', type=str, required=True)\n",
    "    parser.add_argument('--weight_dir', type=str, default=None)\n",
    "    parser.add_argument('--use_lora', action='store_true')\n",
    "    parser.add_argument('--lora_alpha', type=int, default=4)\n",
    "    parser.add_argument('--conv_mode', type=str, default='plain')\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=200)\n",
    "    parser.add_argument('--num_beams', type=int, default=1)\n",
    "    parser.add_argument('--temperature', type=float, default=1.0)\n",
    "    parser.add_argument('--video_caption', action='store_true')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"📦 Loading model...\")\n",
    "    model, processor = load_pllava(\n",
    "        repo_id=args.pretrained_model_name_or_path,\n",
    "        num_frames=args.num_frames,\n",
    "        use_lora=args.use_lora,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        weight_dir=args.weight_dir,\n",
    "    )\n",
    "    model = model.to('npu').eval()\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    chat = ChatPllava(model, processor)\n",
    "\n",
    "    print(\"📽️ Loading video frames...\")\n",
    "    frames = load_video(args.video_path, args.num_frames)\n",
    "    img_list = [frames]  # 必须是二维列表 [ [PIL, PIL, PIL...] ]\n",
    "\n",
    "    print(\"💬 Asking and answering...\")\n",
    "    conv = conv_templates[args.conv_mode].copy()\n",
    "    if args.video_caption:\n",
    "        conv = chat.ask(args.prompt, conv, SYSTEM2)\n",
    "    else:\n",
    "        conv = chat.ask(args.prompt, conv, SYSTEM)\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_message, _, _ = chat.answer(\n",
    "        conv=conv,\n",
    "        img_list=img_list,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n⏱️ Inference took {elapsed:.2f} seconds\")\n",
    "    print(\"\\n===== FINAL ANSWER =====\\n\")\n",
    "    print(llm_message.strip())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要部署gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sh ./scripts/demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以在PLLaVA/tasks/eval/demo/pllava_demo.py文件末尾定义url地址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=10034,\n",
    "    root_path=\"/ai/api/proxy/ascend-k8s/relative/master/30003/proxy/10034\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](https://github.com/PKUHPC/scow-for-ai-tutorial-ascend/raw/tutorial11/tutorial11_PLLaVA视频理解模型推理/gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 作者: 张天戈; 林荣群; 贾川民; 马思伟\n",
    ">\n",
    "> 联系方式: tgzhang@stu.pku.edu.cn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
